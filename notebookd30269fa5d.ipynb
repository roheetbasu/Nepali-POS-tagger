{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13400696,"sourceType":"datasetVersion","datasetId":8504045}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install -q transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/postagger/POS.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datasets\ndatasets.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_list = dataset['labels'].unique().tolist()\nprint(len(label_list))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label2ids = {label:i for i,label in enumerate(label_list)}\nids2label = {values:keys for keys,values in label2ids.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(label2ids)\nprint(ids2label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = dataset[dataset[\"words\"].isna()].index.tolist()\nprint(index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.drop(index=index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(dataset[\"sentence_id\"].unique()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grouped = dataset.groupby(\"sentence_id\")\nprint(len(grouped[\"words\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_grouped = grouped.agg({\n    \"words\":lambda x:list(x),\n    \"labels\":lambda x:list(x)\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_grouped.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train,test = train_test_split(\n    df_grouped,\n    test_size = 0.2,\n    shuffle = True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.reset_index(drop=True,inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"Shushant/nepaliBERT\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train['words'].dtype)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(train[\"words\"]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_input = tokenizer(train.iloc[0][\"words\"],is_split_into_words=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenized_input.keys())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_ids = tokenized_input.word_ids(batch_index=0) \nprint(word_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokens)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\ndataset_train = Dataset.from_pandas(train)\ndataset_test = Dataset.from_pandas(test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(dataset_train['words']))              # datasets.arrow_dataset.Column\nprint(type(dataset_train['words'][0]))           # list\nprint(type(dataset_train['words'][0][0]))        # str\nprint(type(dataset_train['labels'][0]))          # list\nprint(type(dataset_train['labels'][0][0]))       # str or int depending on mapping\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_input = tokenizer(dataset_train[\"words\"][:100],is_split_into_words=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(dataset_train['words'][0])\n# for i,data in enumerate(dataset_train['labels'][0]):\n#     print(f\"{i}:{data}\")\n#     print(label2ids[data])\n    \n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenize_inputs = tokenizer(examples['words'][:],truncation=True,max_length = 512,is_split_into_words=True)\n    labels = []\n\n    for i,label in enumerate(examples['labels'][:]):\n        word_ids = tokenize_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label2ids[label[word_idx]])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n\n    tokenize_inputs['labels'] = labels\n    return tokenize_inputs\n                \n        \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntokenized_inputs = dataset_train.map(tokenize_and_align_labels,batched=True,batch_size=100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}